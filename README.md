
<div align="center">
<h3>Multi-human Interactive Talking Dataset</h3>
<p align="center">
  <a href="https://zeyu-zhu.github.io/webpage/">Zeyu Zhu</a>, 
  <a href="https://sites.google.com/view/showlab">Weijia Wu</a>, 
  <a href="https://scholar.google.com/citations?user=NgjTRe4AAAAJ&hl=zh-CN">Mike Shou Zheng</a>
</p>
<br>
</div>


Official repository for *Muti-human Interactive Talking Dataset*

[Project Website]()  | [Paper]()  | [Dataset]()

<p align="center"><img src="assets/motivation.png" width="800px"/><br> </p>

## ðŸ”¥ News
* We will release the code and dataset within 3 monthes.
* **[2025.5.11]** We initialize the Repo.

## ðŸ’¾ MIT Dataset

<p align="center"><img src="assets/dataset.png" width="800px"/><br> </p>

We present a high-quality dataset for multi-human interactive talking video generation, comprising over \$12\$ hours of high-resolution conversational clips with diverse interaction patterns and approximately \$200\$ distinct identities from two talk shows. These curated videos form the core of our dataset, selected for their natural, engaging interactions, clear speaker dynamics, and avoidance of common visual phenomena in real-world videos such as camera motion, occlusions, and editing artifactsâ€”ensuring clean yet diverse multi-speaker scenarios. This serves as an ideal starting resource for this challenging new task.
